}
mat = matrix(vec, nrow =length(myTable[[1]]), ncol = length(vocab), byrow = T)
colnames(mat) <- vocab
return (mat)
}
## Function to make Document Term Matrix for bigram
## myTable: All text (in all documents) joined together
## Vocab: Vocaulary (bigram)
makeBigramDTM <- function(myTable, vocab){
#vec <- makeVS(myTable$text[[1]], vocab)
vec <- makeBigramVS(myTable[[1]][1], vocab)
if( length(myTable[[1]]) > 1){
for (i in 2:length(myTable[[1]])){
vec <- c(vec, makeBigramVS(myTable[[1]][i], vocab))
}
}
mat = matrix(vec, nrow =length(myTable[[1]]), ncol = length(vocab), byrow = T)
colnames(mat) <- vocab
return (mat)
}
## Function to calculate IDF
## mat: DTM for which IDF is to be determined
getIDF <- function(mat){
idf <- (nrow(mat)/( 1 + colSums( mat  > 0)))
return (idf)
}
## Function to make TF-IDF matrix for query
## mat: DTM for which TF-IDF is to be determined
getQueryTF_IDF <- function(mat){
idf = getIDF(mat)
tf_idf <- sweep(mat, 2, idf, '*')
tf_idf <- replace(tf_idf, tf_idf == 0, 1)
tf_idf <- log(tf_idf)
return (tf_idf)
}
## Function to make TF-IDF matrix
## mat: DTM for which TF-IDF is to be determined
getTF_IDF <- function(mat){
idf = getIDF(mat)
tf_idf <- sweep(mat, 2, idf, '*')
tf_idf <- replace(tf_idf, tf_idf == 0, 1)
tf_idf <- log(tf_idf)
return (tf_idf)
}
## Function to find cosine product between two matrices
## mat1: First input matrix
## mat2: Second input matrix
cosProduct <- function(mat1, mat2){
simil = sum(mat1 * mat2)
k1 = sum(mat1^2)
if (k1 == 0)
return (0)
k2 = sum(mat2^2)
if (k2 == 0)
return (0)
simil = simil/sqrt( k1 * k2 )
return (simil)
}
## Function to calculate score for similarity for a query
## queryVec: Vector in our vector space model for query
## dtm: DTM obtained for corpus
findMatch <- function( queryVec, dtm ){
score <- c( cosProduct( queryVec, dtm[1,]))
for ( i in 2:nrow(dtm) ){
score <- c(score,  cosProduct( queryVec, dtm[i,]) )
}
return (score)
}
## Function to print top 'returnCount' number of matching results
## score: Score vector calculted by taking cosine product of query vectorfor each document with DTM
## returnCount: number of results to be printed
## dataTable: Vector containg all text joined (from all documents)
getResult = function(score, returnCount, dataTable){
sortScore = unique(sort(score, decreasing = TRUE))
#indices <- which( score == max(score) )
indices <- list()
for (i in 1:length(sortScore)){
indices <- c(indices, which(score == sortScore[i], arr.ind = TRUE))
}
# dataframe <- makeRawFrame(dataTable)
count = 0
for (i in indices){
if( count >= returnCount)
break
count = count + 1
printf ("Result %d: \n", count)
print(dataTable[i])
}
}
## Funcion to get scores using results both from similarity and centrality criteria
getPgRank <- function(score, UIDs , centrality){
sortScore = unique(sort(score, decreasing = TRUE))
#indices <- which( score == max(score) )
count = 0
for (i in 1:length(sortScore)){
indices <- c(indices, which(score == sortScore[i], arr.ind = TRUE))
}
retScore <- vector()
count = 1
for (i in indices){
score[i] = score[i]*centrality[(grep(UIDs[i], centrality))]
retScore[count] <- score[i]
count = count + 1
}
return (retScore)
}
## Function to read file in optimum time
## fileName: file to be read
readOptm <- function(fileName){
res <- tolower(readChar(fileName,file.info(fileName)$size))
res <- unlist(strsplit(res, "\r*\n"))
return (res)
}
## Function to read text and summary together
## res: Raw data read form file
readReview <- function( res){
pat <- "^(review/summary)|^(review/text)"
r <- grep(pattern = pat, res, value = TRUE)
r <- lapply(r, function(x) strsplit(x, ':')[[1]][2])
rNew <- lapply(1:(length(r)/2), function(x) paste(r[2*x-1], r[2*x]))
return (rNew)
}
## Generic function to read various attributes from raw data
## res: Raw data read from file
## ptrn: Pattern to be matched using regilar expressions
readInfo <- function(res, ptrn){
r <- grep(pattern = ptrn, res, value = TRUE)
r <- lapply(r, function(x) (strsplit(x, ':')[[1]])[2])
return ( unlist(r) )
}
## Read File
fileOut <- readOptm( "smallData.txt" )
## Get review text
rest1 <- readReview( fileOut)
## Get user IDs
pat <- "^(review/userid)"
uid <- readInfo(fileOut, pat)
## Get product IDs
pat <- "^(product/productid)"
pid <- readInfo(fileOut, pat)
## Get scores/ratings
pat <- "^(review/score)"
rating <- readInfo(fileOut, pat)
## Get helpfulness score
pat <- "^(review/helpfulness)"
helpfl <- readInfo(fileOut, pat)
helpfl <- lapply(strsplit(helpfl, '/'), as.numeric)
helpful <- sapply(helpfl, function(x) x[1]/x[2])
helpful[is.nan(helpful)] <- 0
## Use all data to convert into dataframe
df <- data.frame( author = uid, product = pid, score = rating, helpf = helpful)
## Write all data into a '.csv' file
write.table( df, "data.csv", row.names = FALSE, col.names = FALSE, sep = "," )
## Free the space occupied by dataframe
rm( df )
## Get unique UIDs
uniqUID = unique(uid)
## Get unique PIDs
uniqPID = unique(pid)
## Create and initialize influence matix
influence <- matrix(0, nrow = length(uniqUID), ncol = length(uniqUID))
rownames( influence ) <- uniqUID
colnames( influence ) <- uniqUID
## pid2: PIDs occuring more than once
pid2 <- vector()
pid2 <- unique( uniqPID[which(sapply(uniqPID ,function(x) length(grep(x, pid)) > 1 ), arr.ind = FALSE)] )
## Read data from CSV
myData <- read.csv("data.csv", header=TRUE, sep = ",")
for (i in 1:length(pid2)){
auth <- grep(pid2[i], myData[,2])
# Make combinations
k <- combn(auth, 2)
for (j in 1:ncol(k)){
t <- k[,j]
## Get indices in matrix
t[1] <- grep( uid[t[1]], uniqUID)
t[2] <- grep( uid[t[2]], uniqUID)
## Calculate influence
if( (myData[t[1],3] == 0) || (myData[t[2],3] == 0) || (myData[t[1],4] == 0)){
influence[t[1],t[2]] = influence[t[1],t[2]] + 0.1
}else{
mult = (5 - abs(myData[t[1],3] - myData[t[2],3]))/length(auth)
influence[t[1],t[2]] = influence[t[1],t[2]] + mult*myData[t[1],4]
}
if( (myData[t[1],3] == 0) || (myData[t[2],3] == 0) || (myData[t[2],4] == 0)){
influence[t[2],t[1]] = influence[t[2],t[1]] + 0.1
}else{
mult = (5 - abs(myData[t[1],3] - myData[t[2],3]))/length(auth)
influence[t[2],t[1]] = influence[t[2],t[1]] + mult*myData[t[2],4]
}
}
}
## Get normalized influence matrix
normInfl <- matrix(0, nrow = length(uniqUID), ncol = length(uniqUID))
normInfl <- influence/rowSums(influence)
normInfl[is.nan( normInfl )] = 0
#ranker <-  matrix(runif(0,1), nrow = 1, ncol = length(uniqUID))
ranker <- runif(length(uniqUID), 0, 1)
errorM = matrix(0.00001, nrow = 1, ncol = length(uniqUID))
## Get centrality vector
brek <- 0
while (brek < 0.00001){
r1 = ranker %*% normInfl
brek <- max(abs(r1 - ranker))
ranker <- r1
}
# #write.table(rNew, file = "MyData.csv",row.names=FALSE, na="",col.names=FALSE, sep="\n")
# Convert into dataframes
rest = makeFrame(rest1)
# Make unigram vocaulary
vocab_Uni = makeVocab(rest)
# Make Vector space model with vocabulary
vector_Uni = makeVector(vocab_Uni)
# Make DTM for all documents
DTM_Uni = makeDTM(rest, vocab_Uni)
# Matrix TF-IDF matrix from DTM
tf_idf_Uni = getTF_IDF(DTM_Uni)
# Make bigram vocaulary
vocab_Bi = makeBigramVocab(rest)
# Make Vector space model with vocabulary
vector_Bi = makeVector(vocab_Bi)
# Make DTM for all documents
DTM_Bi = makeBigramDTM(rest, vocab_Bi)
# Matrix TF-IDF matrix from DTM
tf_idf_Bi = getTF_IDF(DTM_Bi)
## Document to Document similarity heatmap
#mat1 <- matrix(0, nrow =  nrow(DTM_Uni), ncol = nrow(DTM_Uni))
#for (i in 1:nrow(DTM_Uni)){
#  for (j in 1:nrow(DTM_Uni)){
#    mat1[i,j] <- cosProduct(DTM_Uni[i,], DTM_Uni[j,])
#  }
#}
# make heatmap for unigram model
#png(file="unigram.png")
#hMapUni <-  heatmap(tf_idf_Uni, Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10))
#dev.off()
#for (i in 1:nrow(DTM_Bi)){
#  for (j in 1:nrow(DTM_Bi)){
#    mat1[i,j] <- cosProduct(DTM_Uni[i,], DTM_Uni[j,])
#  }
#}
# make heatmap for bigram model TF-IDF
#png(file="bigram.png")
#hMapBi <- heatmap(tf_idf_Bi, Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10))
#dev.off()
#### Query #####
## Get query as user input
query <- readline(prompt="Enter single query: ")
## Make data frame from query after processing it
queryRest = makeFrame(query  )
# Make unigram vocaulary for query
queryVocab_Uni = makeVocab(queryRest)
# Get DTM for query for unigrams
queryMat_Uni = makeDTM( queryRest, vocab_Uni)
# Make TF-IDF marix for query (for unigrams)
queryTF_IDF_Uni = getQueryTF_IDF(queryMat_Uni)
# Get score for matching in unigram model for each document
score_Uni = findMatch(queryMat_Uni, DTM_Uni )
# Make bigram vocaulary for query
queryVocab_Bi = makeBigramVocab(queryRest)
# Get DTM for query for bigrams
queryMat_Bi = makeBigramDTM( queryRest, vocab_Bi)
# Make TF-IDF marix for query (for bigrams)
queryTF_IDF_Bi = getQueryTF_IDF(queryMat_Bi)
# Get score for matching in bigram model for each document
score_Bi = findMatch(queryMat_Bi, DTM_Bi)
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
getResult( score, 25, rest1)
score1 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
query <- readline(prompt="Enter single query: ")
queryRest = makeFrame(query  )
# Make unigram vocaulary for query
queryVocab_Uni = makeVocab(queryRest)
queryMat_Uni = makeDTM( queryRest, vocab_Uni)
# Make TF-IDF marix for query (for unigrams)
# Get DTM for query for unigrams
queryTF_IDF_Uni = getQueryTF_IDF(queryMat_Uni)
score_Uni = findMatch(queryMat_Uni, DTM_Uni )
# Get score for matching in unigram model for each document
queryVocab_Bi = makeBigramVocab(queryRest)
# Make bigram vocaulary for query
# Get DTM for query for bigrams
queryMat_Bi = makeBigramDTM( queryRest, vocab_Bi)
# Make TF-IDF marix for query (for bigrams)
queryTF_IDF_Bi = getQueryTF_IDF(queryMat_Bi)
# Get score for matching in bigram model for each document
score_Bi = findMatch(queryMat_Bi, DTM_Bi)
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
# get top 25 simlarity matches for query
score = unlist( alpha*score_Uni + beta*score_Bi)
getResult( score, 25, rest1)
score1 = getPgRank( score, uid , ranker)
## Funcion to get scores using results both from similarity and centrality criteria
getPgRank <- function(score, UIDs , centrality){
sortScore = unique(sort(score, decreasing = TRUE))
#indices <- which( score == max(score) )
indices <- list()
count = 0
for (i in 1:length(sortScore)){
indices <- c(indices, which(score == sortScore[i], arr.ind = TRUE))
}
retScore <- vector()
count = 1
for (i in indices){
score[i] = score[i]*centrality[(grep(UIDs[i], centrality))]
retScore[count] <- score[i]
count = count + 1
}
return (retScore)
}
score1 = getPgRank( score, uid , ranker)
## Funcion to get scores using results both from similarity and centrality criteria
getPgRank <- function(score, UIDs , centrality){
sortScore = unique(sort(score, decreasing = TRUE))
#indices <- which( score == max(score) )
indices <- list()
count = 0
for (i in 1:length(sortScore)){
indices <- c(indices, which(score == sortScore[i], arr.ind = TRUE))
}
retScore <- vector()
count = 1
for (i in indices){
indList <- grep(UIDs[i], centrality)
if( length(indList) > 0){
score[i] = score[i]*centrality[(grep(UIDs[i], centrality))]
}else
score[i] <- 0
retScore[count] <- score[i]
count = count + 1
}
return (retScore)
}
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score1 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
alpha = 1.0
beta = 0.0
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score2 = getPgRank( score, uid , ranker)
#getResult(score, 10, rest1)
d1 <- order(score1)
d2 <- order(score2)
cor.test(d1, d2, method = ("spearman"))
pl <- cor.test(d1, d2, method = ("spearman"))
plot(pl)
pl
d1 <- order(score1)
d2 <- order(score2)
cor.coeff <- cor(d1 , d2 , method = "spearman")
cor.test(d1 , d2 , method = "spearman")
plot(vector1, vector2)
plot(d1, d2)
d1 <- order(score1)
d1
d2 <- order(score2)
d2
cor.coeff <- cor(d1 , d2 , method = "spearman")
cor.test(d1 , d2 , method = "spearman")
d1
d2
### Uni
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 1.0
beta = 0.0
score = unlist( alpha*score_Uni + beta*score_Bi)
# get final score after providing weights to unigram and bigrams
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
#score1 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
dUni <- order(score)
### Uni-Bi
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score2 = getPgRank( score, uid , ranker)
#getResult(score, 10, rest1)
dUniBi <- order(score2)
### UniRank
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 1.0
beta = 0.0
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
#score2 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
dUniRank <- order(score2)
### Uni-Bi Rank
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score2 = getPgRank( score, uid , ranker)
#getResult(score, 10, rest1)
dUniBiRank <- order(score2)
cor.coeff <- cor(dUni , dUniBi , method = "spearman")
cor.test(dUni , dUniBi , method = "spearman")
plot(d1, d2)
cor.coeff <- cor(dUni , dUniRank , method = "spearman")
cor.test(dUni , dUniRank , method = "spearman")
plot(d1, d2)
cor.coeff <- cor(dUniBi , dUniBiRank , method = "spearman")
cor.test(dUniBi , dUniBiRank , method = "spearman")
cor.coeff <- cor(dUni , dUniBi , method = "spearman")
cor.test(dUni , dUniBi , method = "spearman")
plot(dUni, dUniBi)
cor.coeff <- cor(dUni , dUniRank , method = "spearman")
cor.test(dUni , dUniRank , method = "spearman")
plot(dUni, dUniRank)
cor.coeff <- cor(dUniBi , dUniBiRank , method = "spearman")
cor.test(dUniBi , dUniBiRank , method = "spearman")
plot(dUniBi, dUniBiRank)
cor.coeff <- cor(dUni , dUniBi , method = "spearman")
cor.test(dUni , dUniBi , method = "spearman")
plot(dUni, dUniBi)
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
score = unlist( alpha*score_Uni + beta*score_Bi)
# get final score after providing weights to unigram and bigrams
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
#score2 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
dUniBi <- order(score2)
### Uni
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 1.0
beta = 0.0
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
#score1 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
dUni <- order(score)
### Uni-Bi
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
#score2 = getPgRank( score, uid , ranker)
getResult(score, 10, rest1)
dUniBi <- order(score)
### UniRank
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 1.0
beta = 0.0
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score2 = getPgRank( score, uid , ranker)
#getResult(score, 10, rest1)
dUniRank <- order(score2)
### Uni-Bi Rank
# alpha: Weight for unigram model scores
# beta:  Weight for bigram model scores
alpha = 0.3
beta = 0.7
# get final score after providing weights to unigram and bigrams
score = unlist( alpha*score_Uni + beta*score_Bi)
# get top 25 simlarity matches for query
#getResult( score, 25, rest1)
score2 = getPgRank( score, uid , ranker)
#getResult(score, 10, rest1)
dUniBiRank <- order(score2)
cor.coeff <- cor(dUni , dUniBi , method = "spearman")
cor.test(dUni , dUniBi , method = "spearman")
plot(dUni, dUniBi)
cor.coeff <- cor(dUni , dUniRank , method = "spearman")
cor.test(dUni , dUniRank , method = "spearman")
plot(dUni, dUniRank)
cor.coeff <- cor(dUniBi , dUniBiRank , method = "spearman")
cor.test(dUniBi , dUniBiRank , method = "spearman")
plot(dUniBi, dUniBiRank)
